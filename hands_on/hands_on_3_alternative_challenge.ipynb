{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Part 4 \u2014 AntiDote Micro Hands-on (25 minutes)\n\nThis notebook gives you a **live, micro-scale bi-level training loop** that mirrors the paper's adversary-vs-defender dynamics.\n\n## What you will run\n- 8 harmful preference pairs (BeaverTails-style)\n- 8 benign instruction examples (LIMA-style)\n- Defender: LoRA rank = 4\n- Adversary: tiny hidden bottleneck MLP\n- Schedule per block:\n  - 5 adversary steps (log `loss_a`)\n  - 5 defender steps (log `loss_d`, `loss_cap`)\n- 3 blocks total\n\nYou should see `loss_a` fall early, then flatten/rise once the defender starts adapting.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# If needed (e.g., fresh Colab runtime), uncomment:\n# !pip -q install torch transformers peft datasets matplotlib\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import json\nimport math\nimport random\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------\n# Config\n# -----------------\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # ~0.5B\nharmful_path = \"data/dpo_data.json\"\nbenign_path = \"data/instruction_tuning_data.json\"\n\nnum_harmful = 8\nnum_benign = 8\n\nlora_rank = 4\nadversary_hidden_dim = 64\n\nadv_steps_per_block = 5\ndef_steps_per_block = 5\nnum_blocks = 3\n\nmax_length = 256\nlr_adversary = 3e-4\nlr_defender = 2e-4\nlambda_cap = 1.0\nattack_scale = 0.35\n\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"device:\", device)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------\n# Load tiny datasets\n# -----------------\nwith open(harmful_path) as f:\n    harmful_all = json.load(f)\nwith open(benign_path) as f:\n    benign_all = json.load(f)\n\nharmful_data = random.sample(harmful_all, num_harmful)\nbenign_data = random.sample(benign_all, num_benign)\n\nprint(f\"harmful examples: {len(harmful_data)}\")\nprint(f\"benign examples: {len(benign_data)}\")\nprint(\"sample harmful prompt:\", harmful_data[0][\"prompt\"][:120], \"...\")\nprint(\"sample benign prompt:\", benign_data[0][\"prompt\"][:120], \"...\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------\n# Tokenizer/model + defender LoRA\n# -----------------\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=None,\n)\n\nlora_cfg = LoraConfig(\n    r=lora_rank,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(base_model, lora_cfg)\nmodel.to(device)\nmodel.print_trainable_parameters()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------\n# Tiny adversary\n# -----------------\nhidden_size = model.config.hidden_size\n\nclass TinyAdversary(nn.Module):\n    def __init__(self, hidden_size, bottleneck=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(hidden_size, bottleneck),\n            nn.Tanh(),\n            nn.Linear(bottleneck, hidden_size),\n        )\n\n    def forward(self, hidden_states):\n        # hidden_states: [B, T, H]\n        pooled = hidden_states.mean(dim=1)      # [B, H]\n        delta = self.net(pooled)                 # [B, H]\n        return delta.unsqueeze(1)                # [B, 1, H]\n\nadversary = TinyAdversary(hidden_size, adversary_hidden_dim).to(device)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------\n# Utilities\n# -----------------\ndef tokenize_pair(prompt, answer):\n    messages = [\n        {\"role\": \"user\", \"content\": prompt},\n        {\"role\": \"assistant\", \"content\": answer},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False)\n\n    prompt_only = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": prompt}],\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    tok_full = tokenizer(text, truncation=True, max_length=max_length, return_tensors=\"pt\")\n    tok_prompt = tokenizer(prompt_only, truncation=True, max_length=max_length, return_tensors=\"pt\")\n\n    input_ids = tok_full[\"input_ids\"].to(device)\n    attention_mask = tok_full[\"attention_mask\"].to(device)\n\n    labels = input_ids.clone()\n    prompt_len = tok_prompt[\"input_ids\"].shape[1]\n    labels[:, :prompt_len] = -100\n    labels[attention_mask == 0] = -100\n    return input_ids, attention_mask, labels\n\n\ndef sequence_nll_from_logits(logits, labels):\n    # logits: [B, T, V], labels: [B, T]\n    shift_logits = logits[:, :-1, :].contiguous()\n    shift_labels = labels[:, 1:].contiguous()\n\n    loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"sum\")\n    token_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n    valid_tokens = (shift_labels != -100).sum().clamp_min(1)\n    return token_loss / valid_tokens\n\n\ndef forward_with_attack(input_ids, attention_mask):\n    outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        output_hidden_states=True,\n        use_cache=False,\n    )\n    clean_hidden = outputs.hidden_states[-1]\n    attacked_hidden = clean_hidden + attack_scale * adversary(clean_hidden)\n    logits = model.base_model.lm_head(attacked_hidden)\n    return logits\n\n\ndef harmful_losses(example):\n    prompt = example[\"prompt\"]\n\n    # In this dataset: chosen is harmful answer, rejected is safe answer.\n    harmful_answer = example[\"chosen\"]\n    safe_answer = example[\"rejected\"]\n\n    hi, hm, hl = tokenize_pair(prompt, harmful_answer)\n    si, sm, sl = tokenize_pair(prompt, safe_answer)\n\n    harmful_logits = forward_with_attack(hi, hm)\n    safe_logits = forward_with_attack(si, sm)\n\n    nll_harm = sequence_nll_from_logits(harmful_logits, hl)\n    nll_safe = sequence_nll_from_logits(safe_logits, sl)\n\n    # adversary wants harmful easier than safe\n    loss_a = nll_harm - nll_safe\n    # defender wants safe easier than harmful\n    loss_d = nll_safe - nll_harm\n    return loss_a, loss_d\n\n\ndef capability_loss(example):\n    i, m, l = tokenize_pair(example[\"prompt\"], example[\"response\"])\n    outputs = model(input_ids=i, attention_mask=m, use_cache=False)\n    return sequence_nll_from_logits(outputs.logits, l)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------\n# Optimizers + freeze helpers\n# -----------------\ndefender_params = [p for p in model.parameters() if p.requires_grad]\nopt_d = torch.optim.AdamW(defender_params, lr=lr_defender)\nopt_a = torch.optim.AdamW(adversary.parameters(), lr=lr_adversary)\n\n\ndef set_trainable_for_adversary_phase():\n    for p in model.parameters():\n        p.requires_grad = False\n    for p in adversary.parameters():\n        p.requires_grad = True\n\n\ndef set_trainable_for_defender_phase():\n    for p in model.parameters():\n        p.requires_grad = True\n    for p in adversary.parameters():\n        p.requires_grad = False\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------\n# Bi-level micro loop\n# -----------------\nmodel.train()\n\nhistory = {\n    \"step\": [],\n    \"loss_a\": [],\n    \"loss_d\": [],\n    \"loss_cap\": [],\n}\n\nglobal_step = 0\n\nfor block in range(num_blocks):\n    print(f\"\\n===== Block {block+1}/{num_blocks} =====\")\n\n    # 1) Adversary phase\n    set_trainable_for_adversary_phase()\n    for s in range(adv_steps_per_block):\n        ex = random.choice(harmful_data)\n        loss_a, _ = harmful_losses(ex)\n\n        opt_a.zero_grad()\n        loss_a.backward()\n        torch.nn.utils.clip_grad_norm_(adversary.parameters(), 1.0)\n        opt_a.step()\n\n        global_step += 1\n        history[\"step\"].append(global_step)\n        history[\"loss_a\"].append(float(loss_a.detach().cpu()))\n        history[\"loss_d\"].append(float(\"nan\"))\n        history[\"loss_cap\"].append(float(\"nan\"))\n\n        print(f\"Adv step {s+1}/{adv_steps_per_block} | loss_a={loss_a.item():.4f}\")\n\n    # 2) Defender phase\n    set_trainable_for_defender_phase()\n    for s in range(def_steps_per_block):\n        harmful_ex = random.choice(harmful_data)\n        benign_ex = random.choice(benign_data)\n\n        _, loss_d = harmful_losses(harmful_ex)\n        loss_cap = capability_loss(benign_ex)\n        loss_total = loss_d + lambda_cap * loss_cap\n\n        opt_d.zero_grad()\n        loss_total.backward()\n        torch.nn.utils.clip_grad_norm_(defender_params, 1.0)\n        opt_d.step()\n\n        global_step += 1\n        history[\"step\"].append(global_step)\n        history[\"loss_a\"].append(float(\"nan\"))\n        history[\"loss_d\"].append(float(loss_d.detach().cpu()))\n        history[\"loss_cap\"].append(float(loss_cap.detach().cpu()))\n\n        print(\n            f\"Def step {s+1}/{def_steps_per_block} | \"\n            f\"loss_d={loss_d.item():.4f} | loss_cap={loss_cap.item():.4f}\"\n        )\n\n    # Live plot after each block\n    clear_output(wait=True)\n    plt.figure(figsize=(10, 4))\n    plt.plot(history[\"step\"], history[\"loss_a\"], marker=\"o\", label=\"loss_a (adversary)\")\n    plt.plot(history[\"step\"], history[\"loss_d\"], marker=\"o\", label=\"loss_d (defender safety)\")\n    plt.plot(history[\"step\"], history[\"loss_cap\"], marker=\"o\", label=\"loss_cap (defender capability)\")\n    plt.title(\"Micro AntiDote Dynamics\")\n    plt.xlabel(\"micro-step\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.grid(alpha=0.3)\n    plt.show()\n\nprint(\"Done.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Challenge knobs (for groups)\nRun the notebook again with **one** knob changed:\n\n1. `lr_adversary` (e.g., 1e-4, 1e-3)\n2. `lora_rank` (e.g., 2, 8)\n3. `adv_steps_per_block:def_steps_per_block` (e.g., 2:8 or 8:2)\n\nThen report:\n- Did it stabilize?\n- Did adversary dominate?\n- Did defender hold from the start?\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}