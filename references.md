| #  | Title                                                                                                                    | Link                                                                                                          |
| -- | ------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------- |
| 1  | **Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization**           | https://openreview.net/pdf?id=tYRrOdSnVUy                                                                         |
| 2  | **Self-Destructing Models: Increasing the Costs of Harmful Dual Uses in Foundation Models**                              | https://arxiv.org/abs/2211.14946                                                                                                 
| 3  | **Robustifying Safety-Aligned Large Language Models through Clean Data Curation (CTRL)**                                 | [https://arxiv.org/abs/2405.19358](https://arxiv.org/abs/2405.19358)                                        |
| 4  | **Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack**                   | https://proceedings.neurips.cc/paper_files/paper/2024/hash/873c86d9a979ab80d8e2919510d4446b-Abstract-Conference.html |
| 5  | **Improving Alignment and Robustness with Circuit Breakers**                                                             |   https://arxiv.org/abs/2406.04313                                      |
| 6  | **Immunization against harmful fine-tuning attacks**                                                                     |    https://arxiv.org/abs/2402.16382                                                                        |
| 7  | **Ethical Treatment (E.T.) of Language Models Against Harmful Inference-Time Interventions**                             |  https://www.techrxiv.org/users/925680/articles/1301297-ethical-treatment-of-language-models-against-harmful-inference-time-interventions                   
| 8  | **Tamper-Resistant Safeguards for Open-Weight LLMs (TAR)**                                                               |      https://arxiv.org/abs/2408.00761                                                                                                   |
| 9  | **Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation**                 | [https://arxiv.org/abs/2409.01586](https://arxiv.org/abs/2409.01586)                                                   
| 10 | **LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning**                                               |      https://arxiv.org/abs/2506.15606                                                              |
| 11 | **Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning (VAA)**                             |    https://arxiv.org/abs/2506.03850                                                           |
| 12 | **Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning (ILU)**                         |     https://openreview.net/forum?id=x2lm33kdrZ                                                  |
| 13 | **Model Immunization from a Condition Number Perspective**                                                               |       https://openreview.net/forum?id=uitj69FqD5                                                   |
| 14 | **CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning**                           |      https://arxiv.org/abs/2505.16559                                                                                           |
| 15 | **Self-Destructive Language Model (SEAM)**                                                                               |      https://arxiv.org/abs/2505.12186                                                                                            |
| 16 | **Probing the Robustness of Large Language Models Safety to Latent Perturbations (ASA/LAPT)**                            |     https://openreview.net/forum?id=sfz57tKe5E                                                                |
| 17 | **Targeted Vaccine: Safety Alignment for Large Language Models against Harmful Fine-Tuning via Layer-wise Perturbation** | [https://arxiv.org/abs/2410.09760](https://arxiv.org/abs/2410.09760)                                            |
| 18 | **SDD: Self-Degraded Defense against Malicious Fine-tuning**                                                             |    https://arxiv.org/abs/2507.21182                                                     |
| 19 | **AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs**                                                    |  https://arxiv.org/abs/2509.08000                                                                                            |
| 20 | **Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning**                                        |      https://arxiv.org/abs/2508.20697                                                                                        

